현재 상태 요약 (WSL + Docker + Celery + Ollama)

1) 구성 개요
- Next.js: Windows, http://localhost:3000
- FastAPI(main): WSL(Ubuntu), http://127.0.0.1:8000
- Celery Worker: WSL(Ubuntu)
- Celery Bridge: WSL(Ubuntu), http://wsl.localhost:8001
- Redis: Docker on Windows, port 6379 exposed
- LLM: Ollama on WSL, http://127.0.0.1:11434, model=llama3.2:3b

2) 동작 확인 결과
- Ollama(WSL): `curl -s http://127.0.0.1:11434/api/tags` → models에 `llama3.2:3b` 표시
- Redis(Docker): `docker ps` → `redis` 컨테이너, `0.0.0.0:6379->6379/tcp`
- Redis 핑(WSL): `redis-cli -h 127.0.0.1 -p 6379 ping` → PONG
- Celery 로그: `[celery-ollama] base=http://127.0.0.1:11434, model=llama3.2:3b` 확인

3) 환경/설정 스냅샷
- `.env.local`
  - `LLM_PROVIDER=ollama`
  - `OLLAMA_BASE_URL=http://127.0.0.1:11434`
  - `CELERY_API_BASE=http://wsl.localhost:8001` (권장값)
- `dev.config.json`
  - Celery/Bridge는 `runInWsl: true`, Ollama/Redis 값 개발 기본 설정 포함

4) 운영 명령 모음
- Docker Redis 강제 재기동(기본):
  `docker rm -f redis 2>$null; docker run -d --name redis -p 6379:6379 -v redis-data:/data --restart unless-stopped redis:alpine`
- Docker Redis(AOF):
  `docker rm -f redis 2>$null; docker run -d --name redis -p 6379:6379 -v redis-data:/data --restart unless-stopped redis:alpine redis-server --appendonly yes`
- Ollama 서비스(WSL):
  `sudo systemctl enable --now ollama`  또는  `nohup ollama serve >/dev/null 2>&1 &`
- 앱 실행(Windows 루트):
  `npm run dev`

5) 메모/주의 사항
- Next의 `/api/jobs`는 `.env.local`의 `CELERY_API_BASE`를 사용합니다. WSL 브릿지 접근 위해 `http://wsl.localhost:8001` 권장.
- Redis URL은 코드에서 `localhost`로 하드코딩되어 있습니다. 필요 시 환경변수 사용으로 변경하는 소규모 패치 가능.

파일 위치: C:\Users\User\myapp\STATUS_NOW.txt
